# Project 5: Custom CNN Architecture Playground üéÆ

## Overview
**This is YOUR laboratory!** Design, test, and compare different CNN architectures. Learn what works, what doesn't, and most importantly - WHY. This project synthesizes everything you've learned and encourages experimentation.

## What You'll Learn
- ‚úÖ **Architecture design principles**
- ‚úÖ **Comparing multiple models** systematically
- ‚úÖ **Trade-offs** - Accuracy vs Speed vs Parameters
- ‚úÖ **Hyperparameter tuning** strategies
- ‚úÖ **Model selection** based on requirements
- ‚úÖ **Scientific experimentation** approach

## Project Goal

Build and compare different CNN architectures to understand:
1. **Simple** - Fast but potentially lower accuracy
2. **Balanced** - Good trade-off (like your previous projects)
3. **Deep** - Many layers, high capacity
4. **Wide** - Many filters per layer
5. **Residual** - Skip connections for better gradient flow

Then train the best one fully!

## How to Run

```bash
cd project_5_custom
python custom_cnn_playground.py
```

## Configuration

At the top of the script, you can choose:

```python
# Choose your dataset
DATASET_CHOICE = 2  # 1=MNIST, 2=Fashion MNIST, 3=CIFAR-10

# Choose which architectures to test
architectures_to_test = [
    'simple',
    'balanced',
    # 'deep',      # Uncomment to test
    # 'wide',      # Uncomment to test
    # 'residual',  # Uncomment to test
]
```

## Expected Output
- Quick comparison (5 epochs each)
- Architecture comparison table
- Best model selection
- Full training of best model (20 epochs)
- Training time: ~10-20 minutes total

## Files Generated
1. `architecture_comparison.png` - Compare all architectures
2. `custom_cnn_model.h5` - Best model fully trained

## Architecture Options Explained

### 1. üîπ Simple Architecture
```python
Conv2D(32) ‚Üí MaxPool
Conv2D(64) ‚Üí MaxPool
Flatten ‚Üí Dense(64) ‚Üí Output
```

**Pros:**
- ‚ö° Fast training
- ü™∂ Few parameters
- üéØ Good for easy datasets

**Cons:**
- üìâ Lower capacity
- ‚ùå May underfit complex data
- üö´ Less feature learning

**When to use:** Quick prototypes, simple datasets, limited compute

### 2. üîπ Balanced Architecture
```python
Conv(32) ‚Üí BN ‚Üí MaxPool ‚Üí Dropout(0.25)
Conv(64) ‚Üí BN ‚Üí MaxPool ‚Üí Dropout(0.25)
Conv(128) ‚Üí BN
Flatten ‚Üí Dense(128) ‚Üí Dropout(0.5) ‚Üí Output
```

**Pros:**
- ‚öñÔ∏è Great trade-off
- üéØ Good accuracy
- üõ°Ô∏è Prevents overfitting
- ‚è±Ô∏è Reasonable speed

**Cons:**
- ü§∑ Not the absolute best at anything
- üí≠ Middle-of-the-road performance

**When to use:** Most projects, balanced requirements, general purpose

### 3. üîπ Deep Architecture
```python
Conv(32) ‚Üí Conv(32) ‚Üí MaxPool ‚Üí Dropout
Conv(64) ‚Üí Conv(64) ‚Üí MaxPool ‚Üí Dropout
Conv(128) ‚Üí Conv(128) ‚Üí MaxPool ‚Üí Dropout
Flatten ‚Üí Dense(256) ‚Üí Dropout ‚Üí Output
```

**Pros:**
- üéØ High capacity
- üìà Best accuracy potential
- üß† Learns hierarchical features
- üèÜ State-of-the-art approach

**Cons:**
- üêå Slow training
- üíæ Many parameters
- ‚ö†Ô∏è Overfitting risk
- üîå Needs more data

**When to use:** Complex datasets, accuracy priority, have compute power

### 4. üîπ Wide Architecture
```python
Conv(128) ‚Üí BN ‚Üí MaxPool
Conv(256) ‚Üí BN ‚Üí MaxPool
Conv(512) ‚Üí BN
Flatten ‚Üí Dense(256) ‚Üí Dropout ‚Üí Output
```

**Pros:**
- üí™ High capacity per layer
- üéØ Can learn rich features
- üìä Good for diverse data

**Cons:**
- üíæ MANY parameters
- üêå Slow training
- ‚ö†Ô∏è Severe overfitting risk
- üîå Needs LOTS of data

**When to use:** Large datasets, feature-rich problems, powerful hardware

### 5. üîπ Residual Architecture
```python
x ‚Üí Conv ‚Üí Conv ‚Üí (+) ‚Üí Output
    ‚Üì____________‚Üë
    Skip connection
```

**Pros:**
- üåü Advanced technique
- üìà Better gradient flow
- üéØ Can be very deep
- üèÜ Used in ResNet, etc.

**Cons:**
- üß© More complex
- üí≠ Harder to understand
- ‚öôÔ∏è Tricky to implement
- üéöÔ∏è More hyperparameters

**When to use:** Very deep networks, gradient problems, research projects

## Understanding the Comparison

### Metrics Compared

#### 1. **Test Accuracy**
- Most important for performance
- How well does it work on new data?
- Higher is better

#### 2. **Training Time**
- Practical consideration
- How long to iterate?
- Lower is better

#### 3. **Parameter Count**
- Model size
- Memory requirements
- Deployment considerations
- Lower is better (if accuracy same)

#### 4. **Overfitting Gap**
- Train accuracy - Validation accuracy
- Smaller gap = better generalization
- Want close together

### The Comparison Table

```
Architecture    Parameters   Train Acc   Val Acc   Test Acc    Time
-----------------------------------------------------------------
simple          123,456      92.50%      90.20%    89.80%      45.2s
balanced        456,789      95.30%      92.40%    91.90%      78.5s
deep            789,012      97.20%      91.80%    91.50%      145.3s
wide          1,234,567      98.50%      90.10%    89.20%      98.7s
residual        654,321      96.80%      93.50%    92.80%      112.4s
```

### How to Interpret Results

#### Scenario 1: Similar Accuracy
```
simple:    90% in 45s
balanced:  91% in 78s
```
**Choose:** Simple! 1% isn't worth 73% more time

#### Scenario 2: Big Accuracy Difference
```
simple:    85% in 45s
balanced:  92% in 78s
```
**Choose:** Balanced! 7% improvement is significant

#### Scenario 3: Overfitting
```
deep:  Train=98%, Val=88% (10% gap)
balanced: Train=94%, Val=92% (2% gap)
```
**Choose:** Balanced! Better generalization

#### Scenario 4: Production Deployment
```
wide:     92% with 2M parameters
residual: 93% with 600K parameters
```
**Choose:** Residual! Similar accuracy, 3x smaller

## Experiments to Try

### 1. **Test All Architectures**
```python
# Uncomment all in the list
architectures_to_test = [
    'simple',
    'balanced',
    'deep',
    'wide',
    'residual',
]
```

### 2. **Change Dataset**
```python
DATASET_CHOICE = 3  # Try CIFAR-10
```
**Question:** Which architecture wins on harder data?

### 3. **Create Your Own Architecture**
```python
elif architecture_name == 'my_custom':
    model = keras.Sequential([
        # Your design here!
        # Mix and match ideas from other architectures
    ])
```

### 4. **Modify Existing Architectures**

**Make Balanced Deeper:**
```python
# Add another Conv block
keras.layers.Conv2D(256, (3, 3), activation='relu'),
keras.layers.BatchNormalization(),
```

**Make Deep Wider:**
```python
# Increase filter numbers
Conv2D(64) ‚Üí Conv2D(96)
Conv2D(128) ‚Üí Conv2D(192)
```

### 5. **Hyperparameter Grid Search**

Test combinations:
```python
learning_rates = [0.001, 0.0001]
batch_sizes = [32, 64, 128]
dropout_rates = [0.3, 0.5, 0.7]

# Test all combinations
# Record which works best
```

### 6. **Different Optimizers**
```python
# Try different optimizers
optimizer='adam'                                    # Default
optimizer=keras.optimizers.SGD(momentum=0.9)       # Classic
optimizer=keras.optimizers.RMSprop()               # Alternative
optimizer=keras.optimizers.AdamW(weight_decay=0.01) # Modern
```

## Design Principles

### When to Go Deeper (More Layers)
‚úÖ Complex datasets (CIFAR-10, real photos)
‚úÖ Need hierarchical feature learning
‚úÖ Have sufficient data (1000+ samples per class)
‚úÖ Can afford training time

‚ùå Simple patterns (MNIST)
‚ùå Limited data
‚ùå Need fast inference

### When to Go Wider (More Filters)
‚úÖ Rich, diverse features in data
‚úÖ Large dataset
‚úÖ High-resolution images
‚úÖ Computational power available

‚ùå Limited data (will overfit)
‚ùå Simple patterns
‚ùå Memory constraints
‚ùå Mobile deployment

### When to Add Regularization
‚úÖ Overfitting observed (train >> val)
‚úÖ Limited data
‚úÖ Complex model
‚úÖ Better generalization needed

**Regularization techniques:**
- Dropout (0.3-0.7)
- Batch Normalization
- L1/L2 weight regularization
- Data augmentation

### When to Use Skip Connections
‚úÖ Very deep networks (10+ layers)
‚úÖ Vanishing gradient problems
‚úÖ Need to preserve low-level features
‚úÖ Research/advanced projects

‚ùå Simple, shallow networks
‚ùå First projects (adds complexity)

## Common Architecture Patterns

### VGG-style
```python
Conv(64) ‚Üí Conv(64) ‚Üí MaxPool
Conv(128) ‚Üí Conv(128) ‚Üí MaxPool
Conv(256) ‚Üí Conv(256) ‚Üí Conv(256) ‚Üí MaxPool
# Progressive increase in filters
```

### ResNet-style
```python
x = Conv(64)(x)
residual = x
x = Conv(64)(x)
x = Conv(64)(x)
x = Add()([x, residual])
# Skip connections every 2-3 layers
```

### Inception-style
```python
# Multiple filter sizes in parallel
branch1 = Conv2D(64, (1,1))(x)
branch2 = Conv2D(64, (3,3))(x)
branch3 = Conv2D(64, (5,5))(x)
x = Concatenate()([branch1, branch2, branch3])
# Multi-scale feature extraction
```

## Debugging Poor Performance

### Accuracy not improving (<60%)
1. ‚úÖ Check learning rate (try 0.001, 0.0001)
2. ‚úÖ Ensure data is normalized
3. ‚úÖ Check labels are correct
4. ‚úÖ Try simpler architecture first
5. ‚úÖ Increase epochs

### Overfitting (train 95%, val 75%)
1. ‚úÖ Add dropout (0.5)
2. ‚úÖ Add data augmentation
3. ‚úÖ Reduce model size
4. ‚úÖ Get more training data
5. ‚úÖ Add L2 regularization

### Underfitting (both accuracies low)
1. ‚úÖ Increase model capacity
2. ‚úÖ Add more layers
3. ‚úÖ Increase filter numbers
4. ‚úÖ Train for more epochs
5. ‚úÖ Decrease regularization

### Training too slow
1. ‚úÖ Reduce model size
2. ‚úÖ Increase batch size
3. ‚úÖ Use fewer epochs for testing
4. ‚úÖ Reduce image resolution
5. ‚úÖ Use simpler architecture

## Key Takeaways

üí° **No single "best" architecture** - depends on your needs
üí° **Simple often wins** for simple problems
üí° **More parameters ‚â† better** - risk of overfitting
üí° **Balance is key** - accuracy vs speed vs size
üí° **Experiment systematically** - change one thing at a time
üí° **Understand trade-offs** - every choice has consequences

## Your CNN Journey - Complete! üéâ

### What You've Mastered

**Project 1:** Basic CNN (Conv, Pool, Dense)
**Project 2:** Regularization (Dropout, BatchNorm)
**Project 3:** Color images (RGB, deeper networks)
**Project 4:** Data augmentation (transformations)
**Project 5:** Architecture design (systematic comparison)

### You Can Now:
‚úÖ Build CNNs from scratch
‚úÖ Understand each layer's purpose
‚úÖ Prevent overfitting
‚úÖ Handle different image types
‚úÖ Design custom architectures
‚úÖ Compare models systematically
‚úÖ Tune hyperparameters
‚úÖ Debug training issues

## Next Level Challenges

### 1. **Transfer Learning**
Use pre-trained models (VGG16, ResNet50, EfficientNet)
```python
base = keras.applications.ResNet50(weights='imagenet')
```

### 2. **Object Detection**
Find objects in images (YOLO, Faster R-CNN)

### 3. **Semantic Segmentation**
Pixel-level classification (U-Net, DeepLab)

### 4. **GANs**
Generate new images (Generative Adversarial Networks)

### 5. **Attention Mechanisms**
Focus on important parts (Transformers, Vision Transformers)

### 6. **Real-World Deployment**
- Flask/FastAPI web service
- TensorFlow Lite for mobile
- ONNX for cross-platform
- Docker containers

### 7. **Your Final Year Project!**
Apply everything you've learned to solve a real problem

## Questions to Ponder

1. Why does the "simple" architecture sometimes beat "deep" on easy datasets?
2. How do you decide between accuracy and inference speed?
3. What would you change if deploying to a smartphone?
4. How would you design a CNN for medical imaging?
5. What's the relationship between data size and model complexity?

## Final Advice

1. **Understand before optimizing** - Know why it works
2. **Start simple** - Add complexity only when needed
3. **Visualize everything** - See what the model learns
4. **Document experiments** - Track what works
5. **Share knowledge** - Teaching helps you learn
6. **Keep learning** - Field evolves rapidly
7. **Build projects** - Best way to solidify understanding

---

## üéì Congratulations!

You've completed the CNN Practice Projects series! You now have:
- Strong foundation in CNNs
- Practical coding skills
- Ability to design & debug models
- Understanding of key concepts
- Confidence for your final year

**You're ready for real-world projects!** üöÄ

Remember: Every expert started where you are now. Keep experimenting, stay curious, and build amazing things!

---

**Questions? Ideas? Found bugs?**
Review your project files, experiment freely, and most importantly - HAVE FUN! üéâ
